<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143354296-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-143354296-1');
    </script>

    <title>DVGO</title>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>DVGO</title>
    <meta name="keywords" content="DVGO, direct voxel grid optimization, voxel grid, NeRF">
    <meta name="description" content="Fast 3D scenes reconstruction from multiple images. DVGO supports bounded inward-facing, unbounded inward-facing (unbounded 360), and forward-facing capturing.">
    <meta name="google-site-verification" content="zTn5k4uR8M5ulbD97RtehwXhTTfyIYD4zgdiiQsMn6Q" />
    <meta name="msvalidate.01" content="3CC0A964D32EA3E8561E23E2DF370684" />
    <meta property="og:title" content="Direct Voxel Grid Optimiztion">
    <meta name="og:description" content="Fast 3D scenes reconstruction from multiple images. DVGO supports bounded inward-facing, unbounded inward-facing (unbounded 360), and forward-facing capturing.">
    <meta property="og:image" content="https://sunset1995.github.io/dvgo/web_preview.jpeg">
    <meta property="og:image:secure_url" content="https://sunset1995.github.io/dvgo/web_preview.jpeg">
    <meta property="og:image:width" content="600">
    <meta property="og:image:height" content="314">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://sunset1995.github.io/dvgo">
    <meta property="og:site_name" content="DVGO">
    <link rel="icon" href="./favicon.ico">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" integrity="sha384-zCbKRCUGaJDkqS1kPbPd7TveP5iyJE0EjAuZQTgFLD2ylzuqKfdKlfG/eSrtxUkn" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <script src="https://kit.fontawesome.com/e5a777658b.js" crossorigin="anonymous"></script>
</head>
<body>
    <div class="container-lg" style="padding-bottom: 4em;">

        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h1>Forward-facing Fly-through with DVGO</h1>
              <p>This tutorial will guide you to produce smooth forward-facing fly-through video using <a href="https://github.com/sunset1995/DirectVoxGO">DVGO</a> from your casually captured images.</p>
              <p>Below are some of the results from my casual capturing:</p>
<div id="my_casual_demo" class="carousel slide" data-ride="carousel">
  <div class="carousel-inner">
    <div class="carousel-item active" data-interval="8000">
      <video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174271503-9d05a469-0acb-40bd-ab0b-6de45fb123fa.mp4" type="video/mp4" /></video>
      <div class="carousel-caption d-none d-md-block"><h5>Otobai <small>(using 65 photos)</small></h5></div>
    </div>
    <div class="carousel-item" data-interval="8000">
      <video class="img-fluid" loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174271488-0d301dcd-5a26-4813-9872-d82736fc62e0.mp4" type="video/mp4" /></video>
      <div class="carousel-caption d-none d-md-block"><h5>Madoka <small>(using 78 photos)</small></h5></div>
    </div>
  </div>
  <a class="carousel-control-prev" href="#my_casual_demo" role="button" data-slide="prev">
    <span class="carousel-control-prev-icon" aria-hidden="true"></span>
    <span class="sr-only">Previous</span>
  </a>
  <a class="carousel-control-next" href="#my_casual_demo" role="button" data-slide="next">
    <span class="carousel-control-next-icon" aria-hidden="true"></span>
    <span class="sr-only">Next</span>
  </a>
</div>
              <p style="font-size: 0.8rem">Download example images: <a href="https://drive.google.com/file/d/1tXkC6w7ML3A0fsakmvJ9Cgh-6eVdqShl/view?usp=sharing">Otobai.zip</a> and <a href="https://drive.google.com/file/d/1ykrgrp-ayDiZfEAVBzx6Ysm3xmjUD04y/view?usp=sharing">Madoka.zip</a>.</p>
              <p>My working environment: iPhone 6s+ and a desktop computer with a RTX 2080Ti GPU.</p>
            </div>
        </div>


        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h3>How to capture forward-facing scenes?</h3>
              <div class="row">
                <div class="col-6"><img class="img-responsive" src="img/tutor_llff_1.png"></img></div>
                <div class="col-6"><img class="img-responsive" src="img/tutor_llff_2.png"></img></div>
              </div>
              <p style="font-size: 0.8rem">Images credit: <a href="https://github.com/Fyusion/LLFF">Local Light Field Fusion</a>.</p>
              <ul>
                <li>I typically capture 25-100 images in the 2D vertical plane shown above.</li>
                <li>I would lock the camera auto exposure to prevent photometric variation, which can be done by long tap a point of interest on the screen if you use iPhone.</li>
                <li>I would try my best to:</li>
                <ul>
                    <li>Move my camera only on the 2D vertical plane.</li>
                    <li>Fix all my camera viewing angles.</li>
                    <li>Prevent motion blur.</li>
                </ul>
              </ul>
              I would first decide the left/right/bottom/up boundaries on the 2D vertical plane and only move my camera within the boundaries.
              I would slowly move my camera in a zig-zag pattern and takes photos generously.
              I would take more photos after the zig-zag scan before I'm looking too weired...
              The overall capturing process takes about 2 to 5 minutes.
            </div>
        </div>

        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h3>Organize your images</h3>
              <ol>
                <li>Create a folder naming the captured scene with a "<i>source/</i>" folder under it (e.g., <i>data/custom/Otobai/source/</i>).</li>
                <li>Put all your images under the "source" folder.</li>
              </ol>
            </div>
        </div>

        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h3>Structure-from-Motion</h3>
              <p>Run COLMAP (for example, <code>python tools/imgs2poses.py data/custom/Otobai/</code>). After some minutes, you will get the results like below:</p>
              <p><pre>data/custom/Otobai/
  ├── source/  # your source images
  ├── dense/
  │     ├── images/            # undistorted images
  │     ├── images_[2|4|8]/    # downsampled undistorted images
  │     ├── poses_bounds.npy   # inferenced camera poses
  │     ├── poses_perm.npy     # camera id having poses (some may failed)
  │     ├── sparse/            # colmap format camera params and sparse points
  │     └── ... some files if you want to run colmap to inference dense depth
  └── colmap_output.txt, database.db, sparse  # colmap related files
</pre></p>
            </div>
        </div>

        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h3>Run DVGO</h3>
              <p>You first need to create a DVGO config file. Please see <i>configs/custom/Otobai.py</i> and <i>configs/custom/Madoka.py</i> for concrete examples.</p>

              <p>Below show an example config:</p>
              <p><!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #888888"># inherit default setting for custom forward-facing</span>
_base_ <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;./default_forward_facing.py&#39;</span>

<span style="color: #888888"># name your scene</span>
expname <span style="color: #333333">=</span> <span style="background-color: #fff0f0">&#39;MyScene&#39;</span>

data <span style="color: #333333">=</span> <span style="color: #007020">dict</span>(
    datadir<span style="color: #333333">=</span><span style="background-color: #fff0f0">&#39;./data/custom/MyScene/dense&#39;</span>,  <span style="color: #888888"># path to the root of your images</span>
    factor<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2</span>,                               <span style="color: #888888"># the training images resolution</span>
)
</pre></div>
              </p>

              <p>Now we are ready to go: <code>python run.py --config configs/custom/MyScene.py</code>. The overall training takes about 5 to 50 minutes depend on your computation power and the density of the scene.</p>

              <p>Once the training is finished, the model will be saved to <i>logs/custom/MyScene/fine_last.tar</i>.</p>

            </div>
        </div>

        <div class="row mb-3 pt-2">
            <div class="col-md-8 mx-auto">
              <h3>Render fly-through video</h3>
              <p>To preview your fly-through video from the trained model, run <code>python run.py --config configs/custom/MyScene.py --render_only --render_video --render_video_factor 8</code>.</p>
              <p>There are some parameters in your config for you to control the virtual camera trajectory:</p>
              <p><!-- HTML generated using hilite.me --><div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%">data <span style="color: #333333">=</span> <span style="color: #007020">dict</span>(
    movie_render_kwargs<span style="color: #333333">=</span>{
        <span style="background-color: #fff0f0">&#39;scale_r&#39;</span>: <span style="color: #6600EE; font-weight: bold">1.0</span>, <span style="color: #888888"># circling radius</span>
        <span style="background-color: #fff0f0">&#39;scale_f&#39;</span>: <span style="color: #6600EE; font-weight: bold">1.0</span>, <span style="color: #888888"># the distance to the looking point of foucs</span>
        <span style="background-color: #fff0f0">&#39;zdelta&#39;</span>: <span style="color: #6600EE; font-weight: bold">0.5</span>,  <span style="color: #888888"># amplitude of forward motion</span>
        <span style="background-color: #fff0f0">&#39;zrate&#39;</span>: <span style="color: #6600EE; font-weight: bold">1.0</span>,   <span style="color: #888888"># frequency of forward motion</span>
        <span style="background-color: #fff0f0">&#39;N_rots&#39;</span>: <span style="color: #0000DD; font-weight: bold">1</span>,    <span style="color: #888888"># number of rotation in 120 frames</span>
    }
)
</pre></div>
</p>
            <p>You can also use <code>--render_video_flipy</code> to vertical flip the rendered video or use <code>--render_video_rot90 N</code> to rotate the rendered video with N*90 degree.</p>
            <p>The rendered rgb and depth videos are located at <i>logs/custom/MyScene/render_video_fine_last/</i>. You may want to tune the rendering arugments and preview their results. Remove <code>--render_video_factor 8</code> to render high-resolution video after you are happy with the camera trajectory.</p>

            <p>Below demonstrate the effect of some fly-through trajectory parameters.</p>
            <div class="row">
                <div class="col-4 text-center"><p class="m-0">scale_r=0.5</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502205-111f2980-212e-4903-b2f4-435a9b64dde7.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">scale_r=1.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502164-542acb5c-679d-48ac-8374-72a470e1de05.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">scale_r=1.5</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502221-1b763280-4443-4ebf-807d-3ee47b2857c6.mp4" type="video/mp4" /></video></div>
            </div>
            <div class="row">
                <div class="col-4 text-center"><p class="m-0">scale_f=0.5</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502248-97d9bd77-d9a8-461a-a2a3-a4c86e46e4b4.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">scale_f=1.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502164-542acb5c-679d-48ac-8374-72a470e1de05.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">scale_f=10.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502281-50dd3059-d7da-4ba2-a6fc-1323648f4331.mp4" type="video/mp4" /></video></div>
            </div>
            <div class="row">
                <div class="col-4 text-center"><p class="m-0">zdelta=0.5</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502164-542acb5c-679d-48ac-8374-72a470e1de05.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">zdelta=1.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502346-f9a4c975-3d55-4fdf-96f0-11aef00b8599.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">zdelta=2.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502323-2f413723-7016-43d6-9a42-cfea19754e1f.mp4" type="video/mp4" /></video></div>
            </div>
            <div class="row">
                <div class="col-4 text-center"><p class="m-0">zrate=1.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502164-542acb5c-679d-48ac-8374-72a470e1de05.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">zrate=4.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502361-3dfedcc7-8acc-4bef-9808-eb0d9de16cb6.mp4" type="video/mp4" /></video></div>
                <div class="col-4 text-center"><p class="m-0">zrate=12.0</p><video class="img-fluid" autoplay loop muted controls><source src="https://user-images.githubusercontent.com/2712505/174502369-1917feaa-750a-4769-a8f3-c3d80948a51f.mp4" type="video/mp4" /></video></div>
            </div>
            </div>
        </div>

    </div> <!-- container -->

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha384-VHvPCCyXqtD5DqJeNxl2dtTyhF78xXNXdkwX1CZeRusQfRKp+tA7hAShOK/B/fQ2" crossorigin="anonymous"></script>
    <script type="text/javascript">
        // Download video only when active (speedup website loading & network traffic)
        $(".carousel").on('slide.bs.carousel', function(ev) {
            let slides = $(this).find('.carousel-item');
            let vid = slides[ev.to].querySelectorAll('video')[0];
            let isPlaying = vid.currentTime > 0 && vid.readyState > 2;
            if(!isPlaying) {
                vid.play();
            }
        });

        $(window).on('ready load resize', function() {
            $("#my_casual_demo video").height(0.752 * $(".carousel").width());
        });
    </script>
</body>

</html>